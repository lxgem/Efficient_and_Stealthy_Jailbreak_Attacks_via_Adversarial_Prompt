# LLM Attack Template Filtering and Training Project

## Project Overview
This project aims to develop effective language model attack templates through pre-training, template filtering, and distillation training, optimizing for imperceptibility, harmfulness, cheapness, and diversity. The project consists of three main steps: pre-training with `Pre-training.py`, template filtering with `Select.py`, and distillation training with `Llama-DL-Bert.py`.

## Project Structure and Functionality

### 1. Pre-training (`Pre-training.py`)
- **Functionality**: Randomly combines each instruction with a template to generate training samples, using LoRA fine-tuning to pre-train a language model, enhancing its adaptability to attack templates. `Pretrained-Llama` provides pre-trained weights, which can be loaded to accelerate subsequent training or inference.
- **Input**:
  - Template files: JSON format, located in the template directory.
  - Instruction files: JSON format, containing an "instruction" field, located in the instruction directory.
- **Output**: Pre-trained LoRA weights and tokenizer, saved to the specified path.
- **Process**:
  1. Load templates and instructions, randomly combining them to generate prompts.
  2. Process samples using `AutoTokenizer` and `AutoModelForCausalLM` (default: Llama-3.2-1B).
  3. Configure LoRA (r=8, lora_alpha=16, target_modules=["q_proj", "v_proj"]) for fine-tuning.
  4. Train for 10 epochs with a batch size of 4, using the AdamW optimizer (lr=0.0001).

### 2. Template Filtering (`Select.py`)
- **Functionality**: Filters N templates (default N=10) from a template set that satisfy constraints for imperceptibility, harmfulness, cheapness, and diversity.
- **Filtering Criteria**:
  - **Imperceptibility**: Templates do not trigger refusal keywords from the model (Llama2 via Ollama).
  - **Harmfulness**: Responses generated by templates are flagged as harmful by Llama2 (starting with "Yes").
  - **Cheapness**: Token length calculated using BERT tokenizer, prioritizing templates with fewer tokens.
  - **Diversity**: Uses SentenceTransformer (all-MiniLM-L6-v2) to compute cosine similarity of embeddings, selecting templates with the lowest similarity.
- **Input**: JSON file containing templates (with a "template" field).
- **Output**: N filtered templates, saved as a JSON file.
- **Process**:
  1. Load templates and test imperceptibility and harmfulness using the Ollama API (Llama2).
  2. Calculate cheapness for passing templates, initially retaining the top 50.
  3. Use a greedy algorithm to select the N templates with the highest diversity.

### 3. Distillation Training (`Llama-DL-Bert.py`)
- **Functionality**: Trains a BERT model (student model) via knowledge distillation to mimic the behavior of a LLaMA model (teacher model), while optimizing attack template generation.
- **Configuration**:
  - GPT-4o API Key: GPT4O_API_KEY = "sk-zJo23510c5b1fc65f25d03eb2ecf35319224b979250uyShQ"
  - Teacher Model: LLaMA (pre-trained weights require a specified path).
  - Student Model: BERT (bert-base-uncased).
  - Attack Target Model: Vicuna (via Ollama API).
- **Input**:
  - Instruction files: JSON format, containing an "instruction" field.
  - Template files: JSON format, preferably using filtered templates.
-Output: Distillation-trained BERT model weights and statistics (JSON format).
- Process:
  1. Load instructions and templates, randomly combining them to generate prompts.
  2. Process inputs using BERT tokenizer, combining with LLaMA to generate distillation samples.
  3. Train using the RAdam optimizer (lr=1e-3 to 1e-2) and CosineAnnealingLR scheduler.
  4. Evaluate response harmfulness using the GPT-4o API, recording attack success and harmfulness rates.

## Environment Requirements
- Python Version: 3.8+
- Dependencies**:
  - transformers
  - peft
  - torch
  - sentence_transformers
  - requests
  - pandas
  - numpy
  - tqdm

- Hardware: GPU with CUDA support recommended for faster training.